workflow: session-insights-bootstrap-v2
description: Development workflow for session-insights CLI tool (v2 - with decomposition, KPI alignment, and circuit breakers)
agents:
- role: engineer
- role: reviewer
states:
  start:
    type: initial
    auto_transition: engineering
  engineering:
    instructions: |
      Implement the assigned task for the session-insights CLI tool.

      PHASE AWARENESS - YOU ARE IN MEASUREMENT & HARDENING MODE:
      Construction and basic validation are complete (55% KPI progress).
      The remaining 45% is concentrated in measurement infrastructure and
      edge-case hardening, NOT new feature development.

      PRIORITY 1 - AUTOMATED KPI MEASURERS:
      Zero KPIs currently have automated measurers. All progress percentages
      are estimates. If your task can include building or improving a measurer,
      do that FIRST. A measurer is a script or test that outputs a concrete
      metric value (e.g., accuracy %, pass rate, coverage %).

      PRIORITY 2 - EDGE-CASE HARDENING:
      Project detection accuracy has the largest gap (est. 80% vs 95% target).
      Edge cases to test and fix: monorepos, symlinks, nested projects,
      projects without standard markers (no pyproject.toml, no package.json).

      MANDATORY FIRST STEP - MEASURE BEFORE BUILDING:
      Before writing ANY new code, run existing tests and tools:
        uv run pytest tests/ -x -q
        uv run python -m session_insights analyze --dir . --global
      Record the ACTUAL measured values. If a measurer fails on real data,
      fixing it IS your task. Do NOT proceed to new features.

      CLI IMPLEMENTATION SAFETY (HIGH-RISK AREA):
      If your task involves CLI changes, you MUST decompose into:
      1. Argument parsing layer (Click decorators, parameter validation)
      2. Dispatch layer (routing CLI commands to domain functions)
      3. Logic layer (pure functions, independently testable)
      Never implement CLI changes as a single monolithic function.

      BEFORE SIGNALING DONE:
      - All tests pass: `uv run pytest tests/ -x -q`
      - If you built a measurer, show its output with real data
      - If you hardened edge cases, show test results for each edge case
      - Do NOT signal done with failing tests or untested measurers

      Signal "done" when implementation is complete and tests pass.
    on_signal:
      done:
        from: engineer
        next: reviewing
    timeout:
      duration: 30m
      action: nudge
  reviewing:
    instructions: |
      Review the implementation for correctness, quality, and KPI alignment.

      CRITICAL CHECK - MEASUREMENT VALIDATION:
      If the engineer claims to have built or improved a KPI measurer,
      you MUST independently run it and verify the output matches claims.
      Do NOT accept estimated KPI values â€” demand measured values.
      If a measurer does not exist for the KPI being claimed, flag this
      as needs_revision with the specific request to add a measurer.

      Verification steps:
      1. Read all changed files
      2. Run full test suite: `uv run pytest tests/ -x -q --cov=session_insights`
      3. If measurer was added, run it independently and verify output
      4. Test CLI manually if applicable
      5. Check for edge cases and error handling
      6. Verify CLI changes are properly decomposed (not monolithic)

      KPI ALIGNMENT CHECK:
      Does this work actually move a KPI forward? If the task completed
      features but added no measurer and no edge-case coverage, consider
      requesting revision to add measurement.

      Signal "approved" if quality is acceptable AND work is KPI-aligned.
      Signal "needs_revision" with specific feedback if changes needed.
    on_signal:
      approved:
        from: reviewer
        next: finalize
      needs_revision:
        from: reviewer
        next: engineering
    timeout:
      duration: 15m
      action: nudge
  finalize:
    type: terminal
    on_entry:
    - action: commit
